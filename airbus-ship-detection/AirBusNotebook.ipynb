{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"name":"SegmentationAirbusShip","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":66814,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55709,"modelId":76703}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Advanced Computational Techniques for Big Imaging and Signal Data\nThe Airbus Ship Detection Challenge: /kaggle/input/airbus-ship-detection\n\n_A.Y. 2023-2024_\n\n_Alessio De Luca [919790]_","metadata":{"id":"3lljBNU8RxTS"}},{"cell_type":"markdown","source":"## Kaggle function to run on colab","metadata":{"id":"Sw9Mk80zR2bO"}},{"cell_type":"code","source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\n#NOTE: this takes several minutes, if possible run on kaggle\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'airbus-ship-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F9988%2F868324%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240617%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240617T081844Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db7cd8f1060f63ee891f15c1da87af83e90a91a820360881cc45235e7534c09436f7786c8e71c2367e425d9cdfc308d626d9556e0ad633fc67851d4590f2c8a69bcce02bfed9c46321a081c1d807e21ddd7b81896835cfd1eeccfc6db0b2f94afaafcedb4f04502e908d7e66bd9c97eedd7378fc379ec160766ff01b6a968f652198eb9318b73d24a2f65d086069018f0eeda4f7e908378ed99d959a283867e601235ca8bf1c2db9e16813692cbaf85f61f4815a24d4e404952b23ab182f6236cb9f0b76316d0ac9f7d71701407f35b7416e8ff5e6860774b244853aa2f6743563391b97c028f9bba9206e59ce36ce0ac3f66a88ada4e5afe3934d4fe6874ee51'\n\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{"id":"UegIuUasRxTQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Libraries","metadata":{"id":"yRHzB0c7SCo-"}},{"cell_type":"code","source":"!pip install torchsummary\n!pip install segmentation-models-pytorch","metadata":{"id":"zC2d5s7SRxTU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom skimage.measure import label, regionprops\nfrom skimage.color import label2rgb\nfrom skimage.measure import label as skimage_label\nfrom skimage.measure import find_contours\nfrom collections import defaultdict\nimport random\nfrom PIL import Image\n\nimport segmentation_models_pytorch as smp\n\nfrom torchsummary import summary\nfrom torchvision import models\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"YcrUKKhVRxTV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run this if you are using colab\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"hpQDTfJBTpbf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Processing","metadata":{"id":"zkr9K0ksUZVJ"}},{"cell_type":"markdown","source":"### Data Loading","metadata":{"id":"poiD1W8PSFN-"}},{"cell_type":"code","source":"# verify if gpu is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device used:\", device)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"sJy_sr2tRxTV","outputId":"d0a1d3b7-6617-4753-8c26-e20529e267b9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all images from the train set\ntrain_image_dir = \"/kaggle/input/airbus-ship-detection/train_v2/\"\n\n# load datas\ntrain_df = pd.read_csv(\"/kaggle/input/airbus-ship-detection/train_ship_segmentations_v2.csv\")\ntrain_images_paths = glob.glob(train_image_dir + '*.jpg')\nprint(\"Dataset Dimension:\", len(train_images_paths), \"images\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"JtVEJimARxTe","outputId":"ce683b1c-a8b0-4a57-d1b4-63e783d717b5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"U0p7aZ0WRxTf","outputId":"be9308e5-1d8b-4f49-ebcc-56e13a8f1e72","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualize random image shape\nrandom_image_path = np.random.choice(train_images_paths)\n\n\nrandom_image = cv2.imread(random_image_path)\n\nimage_shape = random_image.shape\n\nprint(\"Images shape is:\", image_shape)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"lxCodyLGRxTf","outputId":"2c1e1d8b-16f0-4315-8e16-27ea28aaef26","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Preprocessing","metadata":{"id":"Wj0-pJhNSMG2"}},{"cell_type":"code","source":"# Function to create a random subset (modify seed value to change 'randomness')\ndef create_random_subset(image_paths, subset_fraction, seed=42):\n    np.random.seed(seed)\n    subset_size = int(len(image_paths) * subset_fraction)\n    indices = np.random.choice(len(image_paths), subset_size, replace=False)\n    subset_paths = [image_paths[i] for i in indices]\n    return subset_paths\n\n# Creation of subset\nsubset_fraction = 0.01 # modify to increase/decrease subset fraction (values over 4 % don't work due to RAM limits)\ntrain_paths = create_random_subset(train_images_paths, subset_fraction)\n\nprint(\"Dimension of reduced dataset:\", len(train_paths), \"images\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"zNEd18uyRxTg","outputId":"a5b2e03d-e80f-49a9-c027-69d1a69d7b0a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function that filters images with no ships\ndef filter_images_with_ships(train_df, train_paths, keep_ratio=0.1): #modify keep ratio due increase/decrease number of shipless images\n    images_with_ships = []\n    images_without_ships = []\n\n    for image_path in tqdm(train_paths):\n        image_id = image_path.split('/')[-1]\n        masks = train_df[train_df['ImageId'] == image_id]['EncodedPixels'].values\n        if len(masks) == 1 and pd.isna(masks[0]):\n            images_without_ships.append(image_path)\n        else:\n            images_with_ships.append(image_path)\n\n    n_keep = int(len(images_without_ships) * keep_ratio)\n    filtered_images = images_with_ships + images_without_ships[:n_keep]\n\n    return filtered_images\n\n# Apply filter function\nfiltered_images_paths = filter_images_with_ships(train_df, train_paths, keep_ratio=0.1)\ndelta_images= len(train_paths) - len(filtered_images_paths)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"XVpSgZfnRxTg","outputId":"256a56ac-89d6-4df5-ba7b-5363e9f7e263","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimension of reduced dataset after filter:\", len(filtered_images_paths))\nprint(\"A total of \", delta_images, \"images without ships were removed.\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"yLUpdmQnRxTh","outputId":"7571931e-3a6f-4fe8-9a47-36806e2172e0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to decode csv format masks to images\ndef rle_decode(mask_rle, shape):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"2jZARzjgRxTh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-val-test split\ntrain_paths, val_paths = train_test_split(filtered_images_paths, test_size=0.6, random_state=42)\ntest_paths, val_paths = train_test_split(val_paths, test_size=0.5, random_state=42)\n\nprint(f\"Train set: {len(train_paths)}, Validation set: {len(val_paths)}, Test set: {len(test_paths)}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"fW6h0ybXRxTh","outputId":"d2f64d3b-a8ad-430f-87c2-2d5b4fcedea7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{"id":"VAD1W5SHSU1_"}},{"cell_type":"code","source":"#augmentation functions\ndef apply_augmentation(image, mask, augmentation_type):\n    if augmentation_type == 'flipping':\n        image = cv2.flip(image, 1)\n        mask = cv2.flip(mask, 1)\n    elif augmentation_type == 'rotation':\n        angle = random.choice([90, 180, 270])\n        rows, cols, _ = image.shape\n        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n        image = cv2.warpAffine(image, M, (cols, rows), flags=cv2.INTER_LINEAR)\n        mask = cv2.warpAffine(mask, M, (cols, rows), flags=cv2.INTER_NEAREST)\n    elif augmentation_type == 'zoom':\n        zoom_factor = random.uniform(1.1, 1.5)\n        rows, cols, _ = image.shape\n        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), 0, zoom_factor)\n        image = cv2.warpAffine(image, M, (cols, rows), flags=cv2.INTER_LINEAR)\n        mask = cv2.warpAffine(mask, M, (cols, rows), flags=cv2.INTER_NEAREST)\n    elif augmentation_type == 'clouds':\n        noise = np.random.normal(loc=128, scale=64, size=image.shape).astype(np.uint8)\n        image = cv2.addWeighted(image, 0.5, noise, 0.5, 0)\n    else:\n        raise ValueError(\"Augmentation type not valid\")\n    return image, mask\n\ndef augment_dataset(train_paths, train_df):\n    augmented_images = []\n    augmented_masks = []\n    augmentation_types = ['flipping', 'rotation', 'zoom', 'clouds']\n\n    for image_path in tqdm(train_paths):\n        image_id = image_path.split('/')[-1]\n        image = cv2.imread(image_path)\n        mask_rle = train_df[train_df['ImageId'] == image_id]['EncodedPixels'].values\n        if len(mask_rle) == 1 and pd.isna(mask_rle[0]):\n            mask = np.zeros((image_shape[1], image_shape[0]), dtype=np.uint8)\n        else:\n            mask = np.zeros((image_shape[1], image_shape[0]), dtype=np.uint8)\n            for rle in mask_rle:\n                mask += rle_decode(rle, (image_shape[1], image_shape[0]))\n\n        augmented_images.append(image)\n        augmented_masks.append(mask)\n\n        for aug_type in augmentation_types:\n            aug_image, aug_mask = apply_augmentation(image.copy(), mask.copy(), aug_type)\n            augmented_images.append(aug_image)\n            augmented_masks.append(aug_mask)\n\n    return augmented_images, augmented_masks","metadata":{"id":"SNSHgcCaRxTh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply augmentation only on train dataset\naugmented_images, augmented_masks = augment_dataset(train_paths, train_df)\n","metadata":{"id":"tIwdFxBaRxTi","outputId":"6c4ef66e-752a-4f7a-9506-4d14ff4e29de","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total train images after augmentation: {len(augmented_images)}\")\ntrain_split= len(augmented_images)/(len(augmented_images) + len(val_paths)+ len(test_paths))\nprint(f\"New train-val-split is:\", round(train_split,2)*100,\"% train\", (100-round(train_split,2)*100)/2,\"% val\", (100-round(train_split,2)*100)/2,\"% test\",)","metadata":{"id":"AiBMAHFARxTi","outputId":"4c79e113-e14c-4e69-94ef-595643625c06","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_images_and_masks(images, masks, titles, title):\n    fig, axs = plt.subplots(2, len(images), figsize=(20, 10))\n    for i, (img, mask, ttl) in enumerate(zip(images, masks, titles)):\n        axs[0, i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        axs[0, i].set_title(ttl)\n        axs[0, i].axis('off')\n        axs[1, i].imshow(mask, cmap='gray')\n        axs[1, i].axis('off')\n    plt.suptitle(title)\n    plt.show()\n\ndef show_original_and_augmented_with_masks(image, augmented_images, masks, augmented_masks, titles, title):\n    images_to_show = [image] + augmented_images\n    masks_to_show = [masks] + augmented_masks\n    titles = [\"Immagine originale\"] + titles\n\n    show_images_and_masks(images_to_show, masks_to_show, titles, title)","metadata":{"id":"427wAKDGRxTi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take an example from the dataset and display the original image, augmented images, and associated masks\nrandom_idx = random.randint(0, len(train_paths) - 1)\nimage_path = train_paths[random_idx]\nimage = cv2.imread(image_path)\n\n# Get the mask for the original image\nimage_id = image_path.split('/')[-1]\nmask_rle = train_df[train_df['ImageId'] == image_id]['EncodedPixels'].values\nif len(mask_rle) == 1 and pd.isna(mask_rle[0]):\n    mask = np.zeros((image_shape[1], image_shape[0]), dtype=np.uint8)\nelse:\n    mask = np.zeros((image_shape[1], image_shape[0]), dtype=np.uint8)\n    for rle in mask_rle:\n        mask += rle_decode(rle, (image_shape[1], image_shape[0]))\n\n# Generate augmented images and their masks\naug_images = []\naug_masks = []\ntitles = ['Flipping', 'Rotation', 'Zoom', 'Clouds']\nfor aug_type in titles:\n    aug_image, aug_mask = apply_augmentation(image.copy(), mask.copy(), aug_type.lower())\n    aug_images.append(aug_image)\n    aug_masks.append(aug_mask)\n\n# Display the images and masks with titles\nshow_original_and_augmented_with_masks(image, aug_images, mask, aug_masks, titles, title=\"Original and Augmented Images with Masks\")\n","metadata":{"id":"e1KbYwSwRxTj","outputId":"3886a7ca-12f0-414f-858d-139bfd61eb99","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset creation","metadata":{"id":"yYudrP5fSaiZ"}},{"cell_type":"code","source":"class AugmentedDataset(Dataset):\n    def __init__(self, images, masks, transform=None):\n        self.images = images\n        self.masks = masks\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        mask = self.masks[idx]\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #Ensure image is RGB\n        image = image.astype(np.float32) / 255.0  # Convert image to float32 and normalize\n        mask = mask.astype(np.float32)  # Ensure mask is in float32\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, mask\n\n\nclass OriginalDataset(Dataset):\n    def __init__(self, image_paths, masks_df, transform=None):\n        self.image_paths = image_paths\n        self.masks_df = masks_df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #Ensure image is RGB\n        image = image.astype(np.float32) / 255.0  # Convert image to float32 and normalize\n\n        image_id = image_path.split('/')[-1]\n        masks = self.masks_df[self.masks_df['ImageId'] == image_id]['EncodedPixels'].tolist()\n\n        mask_combined = np.zeros((768, 768), dtype=np.float32)  # Initialize mask as float32\n        if any(pd.notna(m) for m in masks):\n            for mask in masks:\n                if mask is not np.nan:\n                    mask_decoded = rle_decode(mask, (768, 768))\n                    mask_combined += mask_decoded\n\n        if self.transform:\n            image = self.transform(image)\n            mask = mask_combined\n\n        return image, mask\n","metadata":{"id":"FJZvhadjRxTj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n])","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"8wha1NBKRxTj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset creation\naugmented_dataset = AugmentedDataset(augmented_images, augmented_masks, transform=transform)\nval_dataset = OriginalDataset(val_paths, train_df, transform=transform)\ntest_dataset = OriginalDataset(test_paths, train_df, transform=transform)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"g9TdU9rwRxTj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 8\n\n# Create DataLoader\ntrain_loader = DataLoader(augmented_dataset, batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=4)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"ImKiFgxbRxTk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train loader size:\", len(train_loader))\nprint(\"Validation loader size:\", len(val_loader))\nprint(\"Test loader size:\", len(test_loader))","metadata":{"id":"wTCNN8fBRxTk","outputId":"11cea7a5-4097-4708-962d-f2ed53fc7e16","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_batch(loader, title_prefix):\n    batch = next(iter(loader))\n    images = batch[0]\n    masks = batch[1]\n\n    print(\"Batch size of images:\", images.size())\n    print(\"Batch size of masks:\", masks.size())\n\n    fig, axes = plt.subplots(len(images), 2, figsize=(15, len(images) * 5))\n    axes = axes.reshape(-1, 2)  # Reshape axes if it is one-dimensional\n\n    for i in range(len(images)):\n        img = images[i].permute(1, 2, 0).numpy()\n        mask = masks[i].numpy()\n\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title(f\"{title_prefix} Image {i+1}\")\n        axes[i, 0].axis('off')\n\n        axes[i, 1].imshow(mask, cmap='gray')\n        axes[i, 1].set_title(f\"{title_prefix} Mask {i+1}\")\n        axes[i, 1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"dRf2KO63RxTk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize a batch of training data\nvisualize_batch(train_loader, title_prefix=\"Train\")\n\n# # Visualize a batch of validation data\n# visualize_batch(val_loader, title_prefix=\"Validation\")\n\n# # Visualize a batch of test data\n# visualize_batch(test_loader, title_prefix=\"Test\")\n","metadata":{"id":"kUsrT6daRxTk","outputId":"77760793-7a8c-4698-a674-70daade24e69","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ship counting","metadata":{"id":"Cl92QAe8SjZp"}},{"cell_type":"code","source":"# Function to count the number of ships in an image and obtain the colored mask\ndef count_and_color_ships(mask):\n    labeled_mask = label(mask)\n    num_ships = labeled_mask.max()  # The number of labels corresponds to the number of connected components\n    colored_mask = label2rgb(labeled_mask, bg_label=0)\n    return num_ships, colored_mask\n","metadata":{"id":"CRscpgqwRxTk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_EXAMPLES_PER_CLASS = 1  # Set the maximum limit of examples per class\n\ndef get_ship_counts_and_examples(dataset):\n    ship_counts = defaultdict(int)\n    example_images = defaultdict(list)\n\n    # Iterate through the dataset\n    for i in tqdm(range(len(dataset)), desc=\"Processing dataset\"):\n        image, mask = dataset[i]\n        num_ships, colored_mask = count_and_color_ships(mask)\n        ship_counts[num_ships] += 1\n\n        # Add only if the number of examples per class does not exceed the maximum limit\n        if len(example_images[num_ships]) < MAX_EXAMPLES_PER_CLASS:\n            example_images[num_ships].append((image, mask, colored_mask))\n\n    return ship_counts, example_images\n\ndef plot_class_histogram(ship_counts, dataset_name):\n    plt.figure(figsize=(12, 6))\n    colors = plt.cm.tab10.colors[:len(ship_counts)]\n    bars = plt.bar(ship_counts.keys(), ship_counts.values(), color=colors)\n\n    for bar in bars:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom', ha='center')\n\n    plt.xticks(range(max(ship_counts.keys()) + 1))\n    plt.xlabel(\"Number of ships per image\")\n    plt.ylabel(\"Number of images\")\n    plt.title(f\"Distribution of images by number of ships - {dataset_name}\")\n    plt.show()\n\ndef plot_class_examples(example_images, dataset_name):\n    for num_ships in sorted(example_images.keys()):\n        for image_data in example_images[num_ships]:\n            image, original_mask, colored_mask = image_data\n\n            original_image = image.permute(1, 2, 0).numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n            axes[0].imshow(original_image)\n            axes[0].set_title(f\"Original Image - {dataset_name}\")\n            axes[0].axis('off')\n\n            axes[1].imshow(original_mask, cmap='gray')\n            axes[1].set_title(f\"Original Label Mask - {dataset_name}\")\n            axes[1].axis('off')\n\n            axes[2].imshow(colored_mask)\n            axes[2].set_title(f\"Mask with {num_ships} {'ship' if num_ships == 1 else 'ships'} - {dataset_name}\")\n            axes[2].axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\ndef plot_ship_presence_histogram(ship_counts, dataset_name):\n    no_ships = ship_counts[0]\n    one_or_more_ships = sum(count for ships, count in ship_counts.items() if ships > 0)\n    total_images = no_ships + one_or_more_ships\n\n    categories = ['0 Ships', '1+ Ships']\n    values = [no_ships, one_or_more_ships]\n    percentages = [f\"{(v/total_images)*100:.2f}%\" for v in values]\n\n    plt.figure(figsize=(10, 5))\n    bars = plt.bar(categories, values, color=['skyblue', 'orange'])\n\n    for bar, value, percentage in zip(bars, values, percentages):\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, yval/2, f'{int(value)}\\n({percentage})', va='center', ha='center')\n\n    plt.xlabel(\"Categories\")\n    plt.ylabel(\"Number of images\")\n    plt.title(f\"Number of images with 0 ships vs 1+ ships - {dataset_name}\")\n    plt.show()\n","metadata":{"id":"9DHvtb1SRxTl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# count ships and get one sample for each count\nship_counts_train, example_images_train = get_ship_counts_and_examples(augmented_dataset)","metadata":{"id":"U-g4YyEHRxTl","outputId":"80dea13b-323b-4431-c70c-17939b73ee6a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#histogram plotting\nplot_ship_presence_histogram(ship_counts_train, \"Train\")\n\nplot_class_histogram(ship_counts_train, \"Train\")","metadata":{"id":"5KxcIf8-RxTl","outputId":"4d1e3a0a-b6e6-4c63-c591-5aeb0df2ac6c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show images for each ship count\nplot_class_examples(example_images_train, \"Train\")","metadata":{"id":"lbEIUKj1RxTl","outputId":"a22dd617-f9eb-40b7-d9af-e402555bf0f6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# count ships and get one sample for each count\nship_counts_val, example_images_val = get_ship_counts_and_examples(val_dataset)","metadata":{"id":"kPfdclgtRxTm","outputId":"7918dc2b-1915-41c3-eeb6-0484a6894315","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#histogram plotting\nplot_ship_presence_histogram(ship_counts_val, \"Validation\")\n\nplot_class_histogram(ship_counts_val, \"Validation\")\n","metadata":{"id":"yx1Y5vH9RxTm","outputId":"3a15deb0-1e26-473f-84c1-ae996bf8b6aa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show images for each ship count\nplot_class_examples(example_images_val, \"Validation\")","metadata":{"id":"gjFTeQiZRxTm","outputId":"bc27c3ff-6697-4716-b99f-f8cd1c0dd513","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# count ships and get one sample for each count\nship_counts_test, example_images_test = get_ship_counts_and_examples(test_dataset)","metadata":{"id":"GnPCLf1-RxTm","outputId":"b9cbe956-436d-4ad0-dbe7-968832a14468","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#histogram plotting\nplot_ship_presence_histogram(ship_counts_test, \"Test\")\n\nplot_class_histogram(ship_counts_test, \"Test\")","metadata":{"id":"y_gCFObyRxTn","outputId":"f7bb3e24-a110-4e9e-bbd9-fd58553a1e64","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show images for each ship count\nplot_class_examples(example_images_test, \"Test\")","metadata":{"id":"skCEuSIBRxTn","outputId":"2f10a4e1-9f9f-484c-f501-82f648c5d36f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Segmentation","metadata":{"id":"mUicjo7PRxTn"}},{"cell_type":"markdown","source":"### Model definition","metadata":{"id":"dwbMQI_AU4gv"}},{"cell_type":"code","source":"# Create ResNet101 model\nmodel = smp.Unet(\n    encoder_name=\"resnet101\",       \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=1                       \n)\n\n\nmodel = model.to(device)\n\n# Allow all layers to be trainable\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n\n## Optional, freeze part of the decoder \n# for param in model.decoder.parameters():\n#     param.requires_grad = False\n","metadata":{"id":"LpATZ8aQRxTn","outputId":"cfd255bf-e967-4979-899d-d05910f73344","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### RUN THIS CELL TO LOAD THE MODEL WEIGHTS AND AVOID TRAIN/VAL ###\nmodel_path = '/kaggle/input/resnet101deluca/pytorch/resnet101deluca/1/ResNet101.pth' #define your path to the model\n# Load the state_dict\nstate_dict = torch.load(model_path, map_location=torch.device('cpu')) #set device to cpu\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define loss function\ncriterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE)\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)","metadata":{"id":"LV3Vw2j0RxTo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary of the model\nsummary(model, input_size=(3, 768, 768))","metadata":{"id":"e8NIWBQfRxTp","outputId":"71be57f0-7a83-4c20-f672-999c502898e9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a threshold for binary segmentation\nthreshold = 0.5","metadata":{"id":"l3ymPDysRxTp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training and validation","metadata":{"id":"kz6wAz3PS5yO"}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, patience):\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    best_val_loss = float('inf')  # Initialize best_val_loss to infinity\n    best_model_wts = None\n    epochs_no_improve = 0\n\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            images = images.to(device).float()\n            masks = masks.to(device).float().unsqueeze(1)  # Add channel dimension to masks\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            # Calculate accuracy\n            predicted_masks = (outputs > threshold).float()\n            train_correct += (predicted_masks == masks).sum().item()\n            train_total += masks.numel()\n\n        train_loss /= len(train_loader)\n        train_accuracy = train_correct / train_total\n        train_losses.append(train_loss)\n        train_accuracies.append(train_accuracy)\n\n        model.eval()  # Set model to evaluation mode\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, masks in tqdm(val_loader, desc=\"Validation\"):\n                images = images.to(device).float()\n                masks = masks.to(device).float().unsqueeze(1)  # Add channel dimension to masks\n\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                val_loss += loss.item()\n\n                # Calculate accuracy\n                predicted_masks = (outputs > threshold).float()\n                val_correct += (predicted_masks == masks).sum().item()\n                val_total += masks.numel()\n\n        val_loss /= len(val_loader)\n        val_accuracy = val_correct / val_total\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        # Check for improvement\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_wts = model.state_dict()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        # Early stopping\n        if epochs_no_improve >= patience:\n            print(\"Early stopping!\")\n            break\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n    # Load best model weights\n    if best_model_wts is not None:\n        model.load_state_dict(best_model_wts)\n        print(f\"Best model loaded with validation loss: {best_val_loss:.4f}\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies, best_model_wts\n","metadata":{"id":"bkpIZu3oRxTp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train model\ntrain_losses, val_losses, train_accuracies, val_accuracies, best_model_wts = train_model(\n    model, criterion, optimizer, train_loader, val_loader, num_epochs=50, patience=10)","metadata":{"id":"qhJ-eyQaRxTp","outputId":"c431892f-cd1a-46f1-b707-aa21d38b731b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell to save the model\nPATH = '/kaggle/working/ResNet101.pth' #adjust this this path if running on colab\n\n# Save best model\nif best_model_wts is not None:\n    torch.save(best_model_wts, PATH)","metadata":{"id":"lE8xNL4RRxTq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#visualization\ndef plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n\n    # Convert tensors to numpy arrays\n    train_losses_np = np.array(train_losses)\n    val_losses_np = np.array(val_losses)\n    train_accuracies_np = np.array([item.cpu().numpy() if hasattr(item, 'cpu') else item for item in train_accuracies])\n    val_accuracies_np = np.array([item.cpu().numpy() if hasattr(item, 'cpu') else item for item in val_accuracies])\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses_np, 'r-', label='Training Loss')\n    plt.plot(epochs, val_losses_np, 'b-', label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies_np, 'r-', label='Training Accuracy')\n    plt.plot(epochs, val_accuracies_np, 'b-', label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()\n\nplot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)","metadata":{"id":"a0a698O0RxTq","outputId":"61e9953b-a697-40ef-fc5a-88049124752a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run to see the performance on validation images (just to see different images)\n# def visualize_batch(images, masks, predicted_masks):\n#     num_samples = images.size(0)\n\n#     for i in range(num_samples):\n#         plt.figure(figsize=(18, 6))\n\n#         # Original Image\n#         plt.subplot(1, 4, 1)\n#         plt.imshow(images[i].permute(1, 2, 0).cpu())\n#         plt.title('Original Image')\n#         plt.axis('off')\n\n#         # Ground Truth Mask\n#         plt.subplot(1, 4, 2)\n#         plt.imshow(masks[i].squeeze().cpu(), cmap='gray')\n#         plt.title('Ground Truth Mask')\n#         plt.axis('off')\n\n#         # Predicted Mask\n#         plt.subplot(1, 4, 3)\n#         predicted_mask = predicted_masks[i].squeeze().cpu()\n#         predicted_mask_binary = (predicted_mask > threshold).float()  # Apply threshold\n#         plt.imshow(predicted_mask_binary, cmap='gray')\n#         plt.title('Predicted Mask')\n#         plt.axis('off')\n\n#         # Original Image with Overlay Contours\n#         plt.subplot(1, 4, 4)\n#         plt.imshow(images[i].permute(1, 2, 0).cpu())\n#         plt.title('Original Image with Contours')\n#         plt.axis('off')\n\n#         # Find contours in the predicted binary mask\n#         contours = find_contours(predicted_mask_binary.cpu().numpy(), level=0.5)\n\n#         # Draw contours on the original image\n#         for contour in contours:\n#             plt.plot(contour[:, 1], contour[:, 0], linewidth=2, color='red')\n\n#         plt.show()\n\n# def evaluate_model(val_loader, model, device, threshold=threshold):\n#     model.eval()  # Set the model to evaluation mode\n#     with torch.no_grad():  # No gradient calculation during evaluation\n#         for images, masks in val_loader:\n#             images = images.to(device)\n#             masks = masks.to(device)\n\n#             # Forward pass\n#             predicted_masks = model(images)\n\n#             # Visualize a batch of data\n#             visualize_batch(images, masks, predicted_masks, threshold)\n#             break  # Stop after visualizing one batch\n\n# # Usage\n# evaluate_model(val_loader, model, device)\n","metadata":{"id":"dg3XG9yNRxTr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing the model","metadata":{"id":"EL_B_tH7UKGq"}},{"cell_type":"code","source":"def evaluate_model(test_loader, model, device):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # No gradient calculation during evaluation\n        for images, masks in test_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            # Forward pass\n            predicted_masks = model(images)\n\n            # Visualize one batch of data\n            visualize_batch(images, masks, predicted_masks)\n            break  # Stop after visualizing one batch\n\n# Function to visualize a batch of data\ndef visualize_batch(images, masks, predicted_masks):\n    num_samples = images.size(0)\n\n    for i in range(num_samples):\n        plt.figure(figsize=(18, 6))\n\n        # Original image\n        plt.subplot(1, 4, 1)\n        plt.imshow(images[i].permute(1, 2, 0).cpu())\n        plt.title('Original Image')\n        plt.axis('off')\n\n        # Ground truth mask\n        plt.subplot(1, 4, 2)\n        plt.imshow(masks[i].squeeze().cpu(), cmap='gray')\n        plt.title('Original Mask')\n        plt.axis('off')\n\n        # Predicted mask\n        plt.subplot(1, 4, 3)\n        predicted_mask = predicted_masks[i].squeeze().cpu()  # Remove the extra dimension\n        # Apply threshold\n        predicted_mask_binary = (predicted_mask > threshold).float()\n        plt.imshow(predicted_mask_binary, cmap='gray')\n        plt.title('Predicted Mask')\n        plt.axis('off')\n\n        # Original image with overlaid contours\n        plt.subplot(1, 4, 4)\n        plt.imshow(images[i].permute(1, 2, 0).cpu())\n        plt.title('Image with Contours')\n        plt.axis('off')\n\n        # Find contours in the predicted binary mask\n        contours = find_contours(predicted_mask_binary.cpu().numpy(), level=0.5)\n\n        # Draw contours on the original image\n        for contour in contours:\n            plt.plot(contour[:, 1], contour[:, 0], linewidth=2, color='red')\n\n        plt.show()\n\n# Usage\nevaluate_model(test_loader, model, device)\n","metadata":{"id":"do0Tpgs4RxTq","outputId":"0f49f751-0b76-4c5e-a7fa-3e5dfa660221","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Metrics","metadata":{"id":"C4M7k6nuURZn"}},{"cell_type":"code","source":"# Define metric functions\ndef intersection_over_union(pred, target):\n    if torch.sum(target) == 0 and torch.sum(pred) == 0:\n        return 1.0  # Perfect score when there's nothing to predict and model predicts nothing\n    elif torch.sum(target) == 0 and torch.sum(pred) > 0:\n        return 0.0  # Score of 0 when there's nothing to predict but model predicts something\n    else:\n        intersection = torch.logical_and(pred, target).sum()\n        union = torch.logical_or(pred, target).sum()\n        if union == 0:\n            return 0.0  # Return 0.0 instead of NaN when union is 0\n        iou = intersection.float() / union.float()\n        return iou.item()\n\ndef dice_coefficient(pred, target):\n    eps = 1e-9  # to avoid division by zero\n    if torch.sum(target) == 0 and torch.sum(pred) == 0:\n        return 1.0  # Perfect score when there's nothing to predict and model predicts nothing\n    elif torch.sum(target) == 0 and torch.sum(pred) > 0:\n        return 0.0  # Score of 0 when there's nothing to predict but model predicts something\n    else:\n        intersection = torch.sum(pred * target)\n        fsum = torch.sum(pred)\n        ssum = torch.sum(target)\n        dice = (2 * intersection) / (fsum + ssum + eps)\n        return dice.item()\n\ndef precision_score(pred, target):\n    eps = 1e-9  # to avoid division by zero\n    if torch.sum(target) == 0 and torch.sum(pred) == 0:\n        return 1.0  # Perfect score when there's nothing to predict and model predicts nothing\n    elif torch.sum(target) == 0 and torch.sum(pred) > 0:\n        return 0.0  # Score of 0 when there's nothing to predict but model predicts something\n    else:\n        tp = torch.logical_and(pred, target).sum().float()\n        fp = torch.logical_and(pred, 1 - target).sum().float()\n        precision = tp / (tp + fp + eps)\n        return precision.item()\n\ndef recall_score(pred, target):\n    eps = 1e-9  # to avoid division by zero\n    if torch.sum(target) == 0 and torch.sum(pred) == 0:\n        return 1.0  # Perfect score when there's nothing to predict and model predicts nothing\n    elif torch.sum(target) == 0 and torch.sum(pred) > 0:\n        return 0.0  # Score of 0 when there's nothing to predict but model predicts something\n    else:\n        tp = torch.logical_and(pred, target).sum().float()\n        fn = torch.logical_and(1 - pred, target).sum().float()\n        recall = tp / (tp + fn + eps)\n        return recall.item()\n\ndef f1_score(pred, target):\n    p = precision_score(pred, target)\n    r = recall_score(pred, target)\n    f1 = 2 * (p * r) / (p + r + 1e-9)\n    return f1\n\n# Example usage in evaluation pipeline\ndef evaluate_model(test_loader, model, device, threshold):\n    iou_scores = []\n    dice_scores = []\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for images, masks in test_loader:\n            images, masks = images.to(device), masks.to(device)\n\n            # Calculate model predictions\n            predicted_masks = model(images)\n\n            # Apply thresholding to predicted masks\n            predicted_masks_binary = (predicted_masks > threshold).float()\n\n            # Calculate IoU, DICE, Precision, Recall, and F1 for each sample in the batch\n            for i in range(images.size(0)):\n                pred_mask = predicted_masks_binary[i].squeeze()\n                true_mask = masks[i].squeeze()\n\n                iou = intersection_over_union(pred_mask, true_mask)\n                dice = dice_coefficient(pred_mask, true_mask)\n                precision = precision_score(pred_mask, true_mask)\n                recall = recall_score(pred_mask, true_mask)\n                f1 = f1_score(pred_mask, true_mask)\n\n                iou_scores.append(iou)\n                dice_scores.append(dice)\n                precision_scores.append(precision)\n                recall_scores.append(recall)\n                f1_scores.append(f1)\n\n    # Calculate the mean of metrics across all samples in the test dataset\n    mean_iou = sum(iou_scores) / len(iou_scores) if iou_scores else float('nan')\n    mean_dice = sum(dice_scores) / len(dice_scores)\n    mean_precision = sum(precision_scores) / len(precision_scores)\n    mean_recall = sum(recall_scores) / len(recall_scores)\n    mean_f1 = sum(f1_scores) / len(f1_scores)\n\n    # Print the results\n    print(f'Mean IoU: {mean_iou:.4f}')\n    print(f'Mean DICE Coefficient: {mean_dice:.4f}')\n    print(f'Mean Precision: {mean_precision:.4f}')\n    print(f'Mean Recall: {mean_recall:.4f}')\n    print(f'Mean F1 Score: {mean_f1:.4f}')","metadata":{"id":"7AfHKnJDRxTq","outputId":"4d6cef80-fe6e-48ff-9714-a8bfbe2b2ff2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(test_loader, model, device, threshold)","metadata":{"id":"dDktG46YRxTr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Mean IoU: 0.5689\nMean DICE Coefficient: 0.6303\nMean Precision: 0.6805\nMean Recall: 0.6352\nMean F1 Score: 0.6303","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
